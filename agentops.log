2025-12-23 02:26:41,110 - WARNING - [OPENAI INSTRUMENTOR] Error setting up OpenAI streaming wrappers: No module named 'openai.resources.beta.chat'
2025-12-23 02:26:41,119 - INFO - Session Replay for default trace: http://localhost:46691/notavailable/sessions?trace_id=24907220f00e274f81e673232120b7a6
2025-12-23 02:26:41,160 - INFO - You're on the agentops free plan ðŸ¤”
2025-12-23 07:20:29,843 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Request timed out.
2025-12-23 07:23:53,251 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: InternalServerError: Hosted_vllmException - Connection error.. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}
acks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-23 06:22:20,875 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39338 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v2-2000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-23 07:08:17,419 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Request timed out.
2025-12-23 07:23:55,572 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: InternalServerError: Hosted_vllmException - Connection error.. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}
acks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-23 06:15:28,429 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38941 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v2-2000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-23 06:34:18,550 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39169 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v2-2000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v2-2000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
