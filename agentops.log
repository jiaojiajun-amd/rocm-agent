2025-11-29 12:42:11,647 - WARNING - [OPENAI INSTRUMENTOR] Error setting up OpenAI streaming wrappers: No module named 'openai.resources.beta.chat'
2025-11-29 12:42:11,660 - INFO - Session Replay for default trace: http://localhost:47615/notavailable/sessions?trace_id=3c77359ec13a1a5f16173680f7519847
2025-11-29 12:42:11,700 - INFO - You're on the agentops free plan ðŸ¤”
2025-11-30 02:42:45,935 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-11-30 14:00:54,916 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 00:04:25,767 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 07:11:23,890 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 01:28:13,958 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 03:30:22,933 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 05:38:15,195 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 07:38:37,515 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 06:34:42,082 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 06:13:12,353 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-01 07:59:40,612 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
