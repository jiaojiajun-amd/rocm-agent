2025-12-06 10:43:42,986 - WARNING - [OPENAI INSTRUMENTOR] Error setting up OpenAI streaming wrappers: No module named 'openai.resources.beta.chat'
2025-12-06 10:43:42,998 - INFO - Session Replay for default trace: http://localhost:50131/notavailable/sessions?trace_id=9ce0ad7d0174de28e763b266109d7d0f
2025-12-06 10:43:43,069 - INFO - You're on the agentops free plan ðŸ¤”
2025-12-06 18:25:44,211 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-06 18:05:16,359 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=Qwen/Qwen3-8B. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=Qwen/Qwen3-8B\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
