2025-12-25 09:50:06,298 - WARNING - [OPENAI INSTRUMENTOR] Error setting up OpenAI streaming wrappers: No module named 'openai.resources.beta.chat'
2025-12-25 09:50:06,308 - INFO - Session Replay for default trace: http://localhost:39923/notavailable/sessions?trace_id=3c9bd2904d0ab4f4a8572805a287b74a
2025-12-25 09:50:06,336 - INFO - You're on the agentops free plan ðŸ¤”
2025-12-25 13:45:40,582 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38934 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-25 15:53:44,725 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38955 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-25 17:57:59,398 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38966 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-25 20:57:51,826 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39012 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 01:11:19,817 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39047 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 03:17:24,603 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39004 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 05:27:13,066 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39049 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 06:55:00,538 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39053 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 08:16:40,187 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39000 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 10:21:03,186 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38974 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 12:27:41,605 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: InternalServerError: Hosted_vllmException - Connection error.. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}
2025-12-26 12:27:47,942 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 500 - {'error': {'message': 'litellm.InternalServerError: InternalServerError: Hosted_vllmException - Connection error.. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '500'}}
921 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 39022 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 12:17:34,605 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38921 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 11:41:32,637 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38943 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 10:49:29,165 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38993 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
2025-12-26 11:29:24,125 - ERROR - [OPENAI WRAPPER] Error in chat_completion_stream_wrapper: Error code: 400 - {'error': {'message': "litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - This model's maximum context length is 38912 tokens. However, you requested 38975 tokens in the messages, Please reduce the length of the messages. None\nmodel=jsonjiao/qwen3-rocm-sft-v3-3000step. context_window_fallbacks=None. fallbacks=None.\n\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=jsonjiao/qwen3-rocm-sft-v3-3000step\nAvailable Model Group Fallbacks=None", 'type': None, 'param': None, 'code': '400'}}
